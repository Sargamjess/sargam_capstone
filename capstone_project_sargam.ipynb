{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6720790-205e-4df7-ad26-6b332fd5bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"Kaludi/Customer-Support-Responses\")\n",
    "\n",
    "# Combine the query and response into a single text for training\n",
    "data = [f\"Query: {query}\\nResponse: {response}\" for query, response in zip(dataset['train']['query'], dataset['train']['response'])]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.1)\n",
    "\n",
    "# Save the processed data to text files\n",
    "with open('train.txt', 'w') as f:\n",
    "    for item in train_data:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('val.txt', 'w') as f:\n",
    "    for item in val_data:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79add133-e06e-411e-91a8-d283738d5a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 08:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=99, training_loss=1.4870066016611427, metrics={'train_runtime': 540.8764, 'train_samples_per_second': 0.732, 'train_steps_per_second': 0.183, 'total_flos': 25867911168000.0, 'train_loss': 1.4870066016611427, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load the datasets from text files\n",
    "train_dataset = load_dataset('text', data_files={'train': 'train.txt'})\n",
    "val_dataset = load_dataset('text', data_files={'validation': 'val.txt'})\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset['train'],\n",
    "    eval_dataset=tokenized_val_dataset['validation']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b9a2e3-0fb8-4bae-a180-d63e64f6c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I reset my password?\n",
      "Response: We'd be happy to help. Can you please provide your email address so we can send you instructions on how to reset your password?\n",
      "\n",
      "We'd be happy to help. Can you\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query, model, tokenizer, max_length=50):\n",
    "    input_text = f\"Query: {query}\\nResponse:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"How can I reset my password?\"\n",
    "response = generate_response(query, model, tokenizer)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e2dd3d2-82b4-4ffc-9334-57797ef92d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I reset my password?\n",
      "Response: Query: How can I reset my password?\n",
      "Response: We'll notify you of any changes. Can you please provide your email address so we can send you instructions on how to reset your password? Can you please provide your email address so we can\n",
      "\n",
      "\n",
      "Query: What is the status of my order?\n",
      "Response: Query: What is the status of my order?\n",
      "Response: We apologize for the inconvenience. Can you please provide your email address so we can check if there's a problem? Can you please provide your email address so we can check if there's\n",
      "\n",
      "\n",
      "Query: Can I return a product after 30 days?\n",
      "Response: Query: Can I return a product after 30 days?\n",
      "Response: We can return an item for a refund or exchange. Can you please provide the product name or SKU and the product name or SKU you'd like to return it to?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_queries = [\n",
    "    \"How can I reset my password?\",\n",
    "    \"What is the status of my order?\",\n",
    "    \"Can I return a product after 30 days?\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    response = generate_response(query, model, tokenizer)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
